"""–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (v3.0 - Full Data Training)."""

from __future__ import annotations

import json
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, Any, List

# –ò–º–ø–æ—Ä—Ç—ã –∏–∑ —Å–æ—Å–µ–¥–Ω–∏—Ö –º–æ–¥—É–ª–µ–π
from .data_preprocessor import DataPreprocessor
from .feature_engineering import FeatureEngineer
from .model import CoalFireModel
from .metrics import evaluate_model, print_metrics_report
from sklearn.model_selection import TimeSeriesSplit

class CoalCombustionPredictor:
    """
    Orchestrator: Data -> Features -> Model -> Predictions.
    –û–±—É—á–∞–µ—Ç—Å—è –Ω–∞ 100% –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
    """
    
    def __init__(self, data_dir: str | Path, artifacts_dir: str | Path):
        self.data_dir = Path(data_dir)
        self.artifacts_dir = Path(artifacts_dir)
        
        self.artifacts_dir.mkdir(parents=True, exist_ok=True)
        (self.artifacts_dir / "models").mkdir(parents=True, exist_ok=True)
        
        self.preprocessor = DataPreprocessor(self.data_dir)
        self.feature_engineer = FeatureEngineer()
        self.model = CoalFireModel()
        
        self.model_path = self.artifacts_dir / "models" / "coal_fire_model.pkl"
        self.metrics_path = self.artifacts_dir / "training_metrics.json"
        
        if self.model_path.exists():
            try:
                self.model.load(self.model_path)
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
    
    def train(self) -> Dict[str, Any]:
        """
        –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –ü–û–õ–ù–û–ú –¥–∞—Ç–∞—Å–µ—Ç–µ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ 20% –¥–∞–Ω–Ω—ã—Ö.
        """
        print("\n" + "="*60)
        print("üî• –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø –ù–ê 100% –î–ê–ù–ù–´–•")
        print("="*60)
        
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞
        raw_df = self.preprocessor.prepare_full_dataset()
        if raw_df.empty: raise ValueError("‚ùå –î–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç!")

        # 2. –§–∏—á–∏
        full_df = self.feature_engineer.create_features(raw_df)
        
        # 3. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è (0-60 –¥–Ω–µ–π –¥–æ –ø–æ–∂–∞—Ä–∞)
        print("\nüî™ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –≤—ã–±–æ—Ä–∫–∏ (0 <= –¥–Ω–µ–π –¥–æ –ø–æ–∂–∞—Ä–∞ <= 60)...")
        df_model = full_df[
            (full_df['days_until_fire'] >= 0) & 
            (full_df['days_until_fire'] <= 60)
        ].copy()
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞
        df_model = df_model.sort_values('measurement_date')
        
        if len(df_model) < 10: raise ValueError("‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö (<10).")
            
        # 4. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ X –∏ y (–í–°–ï –î–ê–ù–ù–´–ï)
        feature_cols = self.feature_engineer.get_feature_columns()
        for c in feature_cols:
            if c not in df_model.columns: df_model[c] = 0

        X = df_model[feature_cols].fillna(0)
        y = df_model['days_until_fire']
        
        print(f"  üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(X)} —Å—Ç—Ä–æ–∫")
        print(f"  üìÖ –ü–µ—Ä–∏–æ–¥: {df_model['measurement_date'].min().date()} -> {df_model['measurement_date'].max().date()}")

        # 5. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è (Optuna) –Ω–∞ –≤—Å–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ
        # –í–Ω—É—Ç—Ä–∏ Optuna –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Cross-Validation, —Ç–∞–∫ —á—Ç–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ –ø–æ–¥–±–æ—Ä–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–µ –±—É–¥–µ—Ç
        if hasattr(self.model, 'optimize'):
                    try:
                        print("\n‚öôÔ∏è  –ü–æ–¥–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (Optuna) - –ë–´–°–¢–†–´–ô –†–ï–ñ–ò–ú...")
                        # –°–¢–ê–í–ò–ú 5 –í–ú–ï–°–¢–û 20
                        self.model.optimize(X, y, n_trials=5) 
                    except Exception as e:
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ Optuna: {e}")

        # 6. –§–ò–ù–ê–õ–¨–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï (FIT) –ù–ê 100% –î–ê–ù–ù–´–•
        print("\nüí™ –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –ø–æ–ª–Ω–æ–º –æ–±—ä–µ–º–µ...")
        self.model.train_final(X, y)
        
        # 7. –û—Ü–µ–Ω–∫–∞ (Self-Check)
        # –¢–∞–∫ –∫–∞–∫ –º—ã –æ–±—É—á–∏–ª–∏—Å—å –Ω–∞ –≤—Å–µ–º, —Å–º–æ—Ç—Ä–∏–º –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç–æ–º –∂–µ train-—Å–µ—Ç–µ.
        # –≠—Ç–æ –ø–æ–∫–∞–∂–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ –º–æ–¥–µ–ª—å "–≤—ã—É—á–∏–ª–∞ —É—Ä–æ–∫–∏".
        print("\nüìä –ú–ï–¢–†–ò–ö–ò (TRAINING SCORE - –ù–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ –º–æ–¥–µ–ª—å –∑–∞–ø–æ–º–Ω–∏–ª–∞ –¥–∞–Ω–Ω—ã–µ):")
        y_pred = self.model.predict(X)
        metrics = evaluate_model(y.values, y_pred)
        
        print_metrics_report(metrics)
        
        # 8. –ü–æ–ø—ã—Ç–∫–∞ –¥–æ—Å—Ç–∞—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        print("\nüîç –¢–û–ü-10 –ü–†–ò–ó–ù–ê–ö–û–í (Feature Importance):")
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –¥–æ—Å—Ç–∞—Ç—å –∏–∑ XGBoost –Ω–∞–ø—Ä—è–º—É—é
            booster = self.model.model
            if hasattr(booster, 'feature_importances_'):
                imps = booster.feature_importances_
                feats = feature_cols
                fi_df = pd.DataFrame({'feature': feats, 'importance': imps})
                print(fi_df.sort_values('importance', ascending=False).head(10).to_string(index=False))
            else:
                print("  (–ù–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è —Ç–µ–∫—É—â–µ–π –≤–µ—Ä—Å–∏–µ–π –º–æ–¥–µ–ª–∏)")
        except Exception as e:
            print(f"  (–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏: {e})")

        # 9. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ {self.model_path}...")
        self.model.save(self.model_path)
        self._save_metrics(metrics)
        
        return metrics
    
    def predict(self, input_df: pd.DataFrame) -> List[Dict[str, Any]]:
        """–ò–Ω—Ñ–µ—Ä–µ–Ω—Å."""
        if not self.model_path.exists(): raise FileNotFoundError("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")
        df = input_df.copy()
        
        rename_map = {'max_temperature': 'max_temp', 'pile_age_days': 'days_since_formation', 'stack_mass_tons': 'coal_weight'}
        df = df.rename(columns=rename_map)
        
        # –ó–∞–≥–ª—É—à–∫–∏ –¥–ª—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        defaults = {'days_since_formation': 0, 'weather_temp': 10, 'weather_humidity': 70, 'wind_speed_avg': 3, 'coal_weight': 5000}
        for c, v in defaults.items():
            if c not in df.columns: df[c] = v
            
        df_features = self.feature_engineer.create_features(df)
        feature_cols = self.feature_engineer.get_feature_columns()
        for c in feature_cols:
            if c not in df_features.columns: df_features[c] = 0
            
        X = df_features[feature_cols].fillna(0)
        preds_df = self.predict_with_confidence(X)
        
        results = []
        for i, row in df.iterrows():
            results.append({
                'storage_id': str(row.get('storage_id', 'unknown')),
                'stack_id': str(row.get('stack_id', 'unknown')),
                'predicted_ttf_days': float(preds_df.iloc[i]['predicted_days']),
                'risk_level': str(preds_df.iloc[i]['risk_level']),
                'confidence': float(preds_df.iloc[i]['confidence'])
            })
        return results

    def predict_with_confidence(self, X: pd.DataFrame) -> pd.DataFrame:
        predictions = self.model.predict(X)
        predictions = np.maximum(predictions, 0)
        
        # –†–∞—Å—á–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –æ—Ç —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã (—Ñ–∏–∑–∏–∫–∞)
        if 'max_temp' in X.columns:
            temps = X['max_temp'].reset_index(drop=True)
            confidence = 1 / (1 + np.exp(-(temps - 45) / 10))
            confidence = 0.4 + (confidence * 0.55)
        else:
            confidence = pd.Series([0.7] * len(predictions))
            
        risk_level = pd.cut(
            predictions,
            bins=[-1, 7, 14, 30, 60, 10000],
            labels=['–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π', '–≤—ã—Å–æ–∫–∏–π', '—Å—Ä–µ–¥–Ω–∏–π', '–Ω–∏–∑–∫–∏–π', '–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π']
        )
        return pd.DataFrame({'predicted_days': predictions, 'confidence': confidence, 'risk_level': risk_level})

    def _save_metrics(self, metrics: Dict[str, Any]) -> None:
        def sanitize(obj):
            if isinstance(obj, (np.integer, int)): return int(obj)
            elif isinstance(obj, (np.floating, float)): return float(obj)
            elif isinstance(obj, np.ndarray): return sanitize(obj.tolist())
            elif isinstance(obj, dict): return {k: sanitize(v) for k, v in obj.items()}
            return str(obj)
        with open(self.metrics_path, 'w', encoding='utf-8') as f:
            json.dump(sanitize(metrics), f, indent=2, ensure_ascii=False)