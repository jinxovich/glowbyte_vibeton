"""
–ü—Ä–æ—Å—Ç–æ–π –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å —Å–∞–º–æ–≤–æ–∑–≥–æ—Ä–∞–Ω–∏—è —É–≥–ª—è.
–û—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–Ω—Ü–∏–ø–∞—Ö, –±–µ–∑ –ª–∏—à–Ω–∏—Ö –∫–æ—Å—Ç—ã–ª–µ–π.
"""

from __future__ import annotations

import json
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, Any, List
import joblib
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.linear_model import Ridge, ElasticNet
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler


class SimpleCoalFirePredictor:
    """
    –ü—Ä–æ—Å—Ç–æ–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å –ø–æ–∂–∞—Ä–æ–≤.
    –§–æ–∫—É—Å –Ω–∞ –≥–ª–∞–≤–Ω–æ–º: —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞, –≤—Ä–µ–º—è —Ö—Ä–∞–Ω–µ–Ω–∏—è, –≤–ª–∞–∂–Ω–æ—Å—Ç—å.
    """
    
    def __init__(self, data_dir: Path, artifacts_dir: Path):
        self.data_dir = Path(data_dir)
        self.artifacts_dir = Path(artifacts_dir)
        self.artifacts_dir.mkdir(parents=True, exist_ok=True)
        
        self.model = None
        self.scaler = StandardScaler()
        self.model_path = self.artifacts_dir / "models" / "simple_model.pkl"
        self.model_path.parent.mkdir(parents=True, exist_ok=True)
        
    def load_data(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö."""
        print("\nüìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        # 1. Fires - —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
        fires = pd.read_csv(self.data_dir / "fires.csv", encoding='utf-8')
        fires['–î–∞—Ç–∞ –Ω–∞—á–∞–ª–∞'] = pd.to_datetime(fires['–î–∞—Ç–∞ –Ω–∞—á–∞–ª–∞'])
        fires['–ù–∞—á.—Ñ–æ—Ä–º.—à—Ç–∞–±–µ–ª—è'] = pd.to_datetime(fires['–ù–∞—á.—Ñ–æ—Ä–º.—à—Ç–∞–±–µ–ª—è'])
        fires = fires.rename(columns={
            '–°–∫–ª–∞–¥': 'storage_id',
            '–®—Ç–∞–±–µ–ª—å': 'stack_id',
            '–î–∞—Ç–∞ –Ω–∞—á–∞–ª–∞': 'fire_date',
            '–ù–∞—á.—Ñ–æ—Ä–º.—à—Ç–∞–±–µ–ª—è': 'formation_date'
        })
        print(f"  ‚úì –ü–æ–∂–∞—Ä—ã: {len(fires)} —Å–æ–±—ã—Ç–∏–π")
        
        # 2. Temperature - –≥–ª–∞–≤–Ω—ã–π –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä
        temp = pd.read_csv(self.data_dir / "temperature.csv", encoding='utf-8')
        temp['–î–∞—Ç–∞ –∞–∫—Ç–∞'] = pd.to_datetime(temp['–î–∞—Ç–∞ –∞–∫—Ç–∞'])
        temp = temp.rename(columns={
            '–°–∫–ª–∞–¥': 'storage_id',
            '–®—Ç–∞–±–µ–ª—å': 'stack_id',
            '–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞': 'max_temp',
            '–î–∞—Ç–∞ –∞–∫—Ç–∞': 'measurement_date'
        })
        print(f"  ‚úì –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–Ω—ã–µ –∑–∞–º–µ—Ä—ã: {len(temp)} –∑–∞–ø–∏—Å–µ–π")
        
        # 3. Supplies - –º–∞—Å—Å–∞ –∏ –≤—Ä–µ–º—è —Ö—Ä–∞–Ω–µ–Ω–∏—è
        supplies = pd.read_csv(self.data_dir / "supplies.csv", encoding='utf-8')
        supplies['–í—ã–≥—Ä—É–∑–∫–∞–ù–∞–°–∫–ª–∞–¥'] = pd.to_datetime(supplies['–í—ã–≥—Ä—É–∑–∫–∞–ù–∞–°–∫–ª–∞–¥'])
        supplies = supplies.rename(columns={
            '–°–∫–ª–∞–¥': 'storage_id',
            '–®—Ç–∞–±–µ–ª—å': 'stack_id',
            '–ù–∞ —Å–∫–ª–∞–¥, —Ç–Ω': 'mass_tons'
        })
        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –ø–æ —à—Ç–∞–±–µ–ª—é
        supplies_agg = supplies.groupby(['storage_id', 'stack_id']).agg({
            'mass_tons': 'sum',
            '–í—ã–≥—Ä—É–∑–∫–∞–ù–∞–°–∫–ª–∞–¥': 'min'
        }).reset_index()
        supplies_agg = supplies_agg.rename(columns={'–í—ã–≥—Ä—É–∑–∫–∞–ù–∞–°–∫–ª–∞–¥': 'first_unload_date'})
        print(f"  ‚úì –ü–æ—Å—Ç–∞–≤–∫–∏: {len(supplies_agg)} —à—Ç–∞–±–µ–ª–µ–π")
        
        # 4. Weather - –≤–ª–∞–∂–Ω–æ—Å—Ç—å –∏ –≤–µ—Ç–µ—Ä
        weather_files = list(self.data_dir.glob("weather_data_*.csv"))
        if weather_files:
            weather_dfs = []
            for f in weather_files:
                df = pd.read_csv(f, encoding='utf-8')
                weather_dfs.append(df)
            weather = pd.concat(weather_dfs, ignore_index=True)
            weather['date'] = pd.to_datetime(weather['date'])
            # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –ø–æ –¥–Ω—é
            weather_daily = weather.groupby(weather['date'].dt.date).agg({
                't': 'mean',
                'humidity': 'mean',
                'v_avg': 'mean',
                'precipitation': 'sum'
            }).reset_index()
            weather_daily['date'] = pd.to_datetime(weather_daily['date'])
            weather_daily = weather_daily.rename(columns={
                't': 'air_temp',
                'v_avg': 'wind_speed'
            })
            print(f"  ‚úì –ü–æ–≥–æ–¥–∞: {len(weather_daily)} –¥–Ω–µ–π")
        else:
            weather_daily = pd.DataFrame()
        
        # –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï
        print("\nüîó –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
        
        # Normalize IDs
        for df in [fires, temp, supplies_agg]:
            df['storage_id'] = df['storage_id'].astype(str).str.strip()
            df['stack_id'] = df['stack_id'].astype(str).str.strip()
        
        # –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã + –ø–æ—Å—Ç–∞–≤–∫–∏
        data = temp.merge(supplies_agg, on=['storage_id', 'stack_id'], how='left')
        
        # + –ü–æ–≥–æ–¥–∞ (–ø–æ –¥–∞—Ç–µ –∑–∞–º–µ—Ä–∞)
        if not weather_daily.empty:
            data['weather_date'] = data['measurement_date'].dt.date
            data['weather_date'] = pd.to_datetime(data['weather_date'])
            data = data.merge(weather_daily, left_on='weather_date', right_on='date', how='left')
            data = data.drop(columns=['date', 'weather_date'])
        
        # + –ü–æ–∂–∞—Ä—ã (–ø—Ä–∏–≤—è–∑–∫–∞ –∫ –ë–õ–ò–ñ–ê–ô–®–ï–ú–£ –±—É–¥—É—â–µ–º—É –ø–æ–∂–∞—Ä—É)
        print("  üîó –ü—Ä–∏–≤—è–∑—ã–≤–∞–µ–º –∏–∑–º–µ—Ä–µ–Ω–∏—è –∫ –±–ª–∏–∂–∞–π—à–∏–º –ø–æ–∂–∞—Ä–∞–º...")
        
        # –ö–∞–∂–¥–æ–µ –∏–∑–º–µ—Ä–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –ø—Ä–∏–≤—è–∑—ã–≤–∞—Ç—å—Å—è –∫ –±–ª–∏–∂–∞–π—à–µ–º—É –ë–£–î–£–©–ï–ú–£ –ø–æ–∂–∞—Ä—É
        data_with_fires = []
        
        for (storage, stack), temp_group in data.groupby(['storage_id', 'stack_id']):
            # –ü–æ–∂–∞—Ä—ã —ç—Ç–æ–≥–æ —à—Ç–∞–±–µ–ª—è
            stack_fires = fires[
                (fires['storage_id'] == storage) & 
                (fires['stack_id'] == stack)
            ].sort_values('fire_date')
            
            if len(stack_fires) == 0:
                continue
            
            # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–º–µ—Ä–µ–Ω–∏—è –Ω–∞—Ö–æ–¥–∏–º –±–ª–∏–∂–∞–π—à–∏–π –±—É–¥—É—â–∏–π –ø–æ–∂–∞—Ä
            for _, measurement in temp_group.iterrows():
                meas_date = measurement['measurement_date']
                
                # –ü–æ–∂–∞—Ä—ã –ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ –∏–∑–º–µ—Ä–µ–Ω–∏—è
                future_fires = stack_fires[stack_fires['fire_date'] > meas_date]
                
                if len(future_fires) > 0:
                    # –ë–µ—Ä–µ–º –ë–õ–ò–ñ–ê–ô–®–ò–ô –ø–æ–∂–∞—Ä
                    nearest_fire = future_fires.iloc[0]
                    
                    measurement['fire_date'] = nearest_fire['fire_date']
                    measurement['formation_date'] = nearest_fire['formation_date']
                    
                    data_with_fires.append(measurement)
        
        if not data_with_fires:
            raise ValueError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –ø—Ä–∏–≤—è–∑–∫–∏ –∫ –ø–æ–∂–∞—Ä–∞–º!")
        
        data = pd.DataFrame(data_with_fires)
        
        # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: –¥–Ω–∏ –¥–æ –ø–æ–∂–∞—Ä–∞
        data['days_to_fire'] = (data['fire_date'] - data['measurement_date']).dt.days
        
        # –î–Ω–∏ —Ö—Ä–∞–Ω–µ–Ω–∏—è
        data['storage_days'] = (data['measurement_date'] - data['formation_date']).dt.days
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º: —Ç–æ–ª—å–∫–æ –∏–∑–º–µ—Ä–µ–Ω–∏—è –ø–µ—Ä–µ–¥ –ø–æ–∂–∞—Ä–æ–º (0-60 –¥–Ω–µ–π)
        data = data[
            (data['days_to_fire'] >= 0) & 
            (data['days_to_fire'] <= 60)
        ].copy()
        
        print(f"  ‚úì –ò—Ç–æ–≥–æ –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {len(data)}")
        
        return data
    
    def create_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –ü–†–û–°–¢–´–• –∏ –ü–û–ù–Ø–¢–ù–´–• –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
        –¢–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ —Ä–µ–∞–ª—å–Ω–æ –≤–ª–∏—è–µ—Ç –Ω–∞ —Å–∞–º–æ–≤–æ–∑–≥–æ—Ä–∞–Ω–∏–µ.
        """
        df = df.copy()
        
        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤
        df['max_temp'] = df['max_temp'].fillna(df['max_temp'].median())
        df['mass_tons'] = df['mass_tons'].fillna(5000)
        df['storage_days'] = df['storage_days'].fillna(0)
        df['humidity'] = df['humidity'].fillna(50)
        df['air_temp'] = df['air_temp'].fillna(15)
        df['wind_speed'] = df['wind_speed'].fillna(3)
        df['precipitation'] = df['precipitation'].fillna(0)
        
        # ===== –ö–õ–Æ–ß–ï–í–´–ï –ü–†–ò–ó–ù–ê–ö–ò =====
        
        # 1. –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (–ì–õ–ê–í–ù–´–ô —Ñ–∞–∫—Ç–æ—Ä)
        df['temp'] = df['max_temp']
        df['temp_squared'] = df['max_temp'] ** 2
        df['temp_cubed'] = df['max_temp'] ** 3
        
        # 2. –í–æ–∑—Ä–∞—Å—Ç —à—Ç–∞–±–µ–ª—è
        df['age_days'] = df['storage_days']
        df['age_weeks'] = df['storage_days'] / 7
        df['age_squared'] = df['storage_days'] ** 2
        
        # 3. –ú–∞—Å—Å–∞ (–±–æ–ª—å—à–µ –º–∞—Å—Å–∞ = –º–µ–¥–ª–µ–Ω–Ω–µ–µ –æ—Å—Ç—ã–≤–∞–µ—Ç)
        df['mass'] = df['mass_tons']
        df['log_mass'] = np.log1p(df['mass_tons'])
        
        # 4. –í–ª–∞–∂–Ω–æ—Å—Ç—å (—Å—É—Ö–æ–π —É–≥–æ–ª—å = –æ–ø–∞—Å–Ω–µ–µ)
        df['humidity_pct'] = df['humidity']
        df['dryness'] = 100 - df['humidity']
        
        # 5. –í–µ—Ç–µ—Ä (—É—Å–∏–ª–∏–≤–∞–µ—Ç –æ–∫–∏—Å–ª–µ–Ω–∏–µ)
        df['wind'] = df['wind_speed']
        
        # 6. –û—Å–∞–¥–∫–∏ (–º–æ–∫—Ä—ã–π —É–≥–æ–ª—å = –±–µ–∑–æ–ø–∞—Å–Ω–µ–µ)
        df['rain'] = df['precipitation']
        
        # ===== –í–ó–ê–ò–ú–û–î–ï–ô–°–¢–í–ò–Ø =====
        
        # –¢–µ—Ä–º–∏—á–µ—Å–∫–∞—è –æ–ø–∞—Å–Ω–æ—Å—Ç—å
        df['thermal_risk'] = df['max_temp'] * df['storage_days']
        
        # –≠—Ñ—Ñ–µ–∫—Ç —Å—É—Ö–æ—Å—Ç–∏
        df['dry_heat'] = df['max_temp'] * df['dryness']
        
        # –û–∫–∏—Å–ª–µ–Ω–∏–µ
        df['oxidation'] = df['max_temp'] * df['wind_speed'] * (100 - df['humidity']) / 100
        
        # –¢–µ–ø–ª–æ–≤–∞—è –∏–Ω–µ—Ä—Ü–∏—è (–±–æ–ª—å—à–∞—è –º–∞—Å—Å–∞ –¥–æ–ª—å—à–µ –¥–µ—Ä–∂–∏—Ç —Ç–µ–ø–ª–æ)
        df['thermal_mass'] = df['max_temp'] * np.log1p(df['mass_tons'])
        
        # –í—Ä–µ–º—è √ó –º–∞—Å—Å–∞
        df['age_mass'] = df['storage_days'] * np.log1p(df['mass_tons'])
        
        # –ü–æ—Ä–æ–≥ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–π —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã
        df['critical_temp'] = (df['max_temp'] > 60).astype(int)
        df['high_temp'] = (df['max_temp'] > 45).astype(int)
        df['warm_temp'] = (df['max_temp'] > 35).astype(int)
        
        # –í—Ä–µ–º—è —Ö—Ä–∞–Ω–µ–Ω–∏—è (–æ–ø–∞—Å–Ω—ã–µ –ø–æ—Ä–æ–≥–∏)
        df['old_pile'] = (df['storage_days'] > 30).astype(int)
        df['very_old_pile'] = (df['storage_days'] > 60).astype(int)
        
        return df
    
    def get_feature_names(self) -> List[str]:
        """–°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏."""
        return [
            # –ë–∞–∑–æ–≤—ã–µ
            'temp', 'temp_squared', 'temp_cubed',
            'age_days', 'age_weeks', 'age_squared',
            'mass', 'log_mass',
            'humidity_pct', 'dryness',
            'wind', 'rain',
            # –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
            'thermal_risk', 'dry_heat', 'oxidation',
            'thermal_mass', 'age_mass',
            # –ü–æ—Ä–æ–≥–∏
            'critical_temp', 'high_temp', 'warm_temp',
            'old_pile', 'very_old_pile'
        ]
    
    def train(self) -> Dict[str, Any]:
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏."""
        print("\n" + "="*70)
        print("üî• –û–ë–£–ß–ï–ù–ò–ï –ü–†–û–°–¢–û–ô –ú–û–î–ï–õ–ò")
        print("="*70)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞
        data = self.load_data()
        
        if len(data) < 50:
            raise ValueError(f"–°–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö: {len(data)}")
        
        # –ü—Ä–∏–∑–Ω–∞–∫–∏
        print("\nüîß –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        data = self.create_features(data)
        
        feature_names = self.get_feature_names()
        X = data[feature_names].fillna(0)
        y = data['days_to_fire']
        
        print(f"  ‚úì –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_names)}")
        print(f"  ‚úì –ü—Ä–∏–º–µ—Ä–æ–≤: {len(X)}")
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
        X_scaled = self.scaler.fit_transform(X)
        
        # –ú–æ–¥–µ–ª—å - Random Forest —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π
        print("\nü§ñ –û–±—É—á–µ–Ω–∏–µ Random Forest (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è)...")
        self.model = RandomForestRegressor(
            n_estimators=200,          # –ë–æ–ª—å—à–µ –¥–µ—Ä–µ–≤—å–µ–≤
            max_depth=6,               # –ù–µ–º–Ω–æ–≥–æ –≥–ª—É–±–∂–µ (–±—ã–ª–æ 3)
            min_samples_split=10,      # –ú–µ–Ω—å—à–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π (–±—ã–ª–æ 20)
            min_samples_leaf=5,        # –ú–µ–Ω—å—à–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π (–±—ã–ª–æ 10)
            max_features='sqrt',       # –ö–æ—Ä–µ–Ω—å –∏–∑ —á–∏—Å–ª–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            random_state=42,
            n_jobs=-1
        )
        
        # Cross-validation
        tscv = TimeSeriesSplit(n_splits=5)
        cv_scores = []
        
        for fold, (train_idx, val_idx) in enumerate(tscv.split(X_scaled), 1):
            X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
            
            self.model.fit(X_train, y_train)
            y_pred = self.model.predict(X_val)
            
            # –ú–µ—Ç—Ä–∏–∫–∏
            mae = np.mean(np.abs(y_pred - y_val))
            accuracy_2d = np.mean(np.abs(y_pred - y_val) <= 2) * 100
            
            cv_scores.append(accuracy_2d)
            print(f"  Fold {fold}: Accuracy ¬±2d = {accuracy_2d:.1f}%, MAE = {mae:.2f}")
        
        print(f"\n  ‚úì –°—Ä–µ–¥–Ω—è—è CV Accuracy: {np.mean(cv_scores):.1f}%")
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
        print("\nüíæ –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ...")
        self.model.fit(X_scaled, y)
        
        # Feature importance
        if hasattr(self.model, 'feature_importances_'):
            importance = pd.DataFrame({
                'feature': feature_names,
                'importance': self.model.feature_importances_
            }).sort_values('importance', ascending=False)
            
            print("\nüîç –¢–û–ü-10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
            print(importance.head(10).to_string(index=False))
        elif hasattr(self.model, 'coef_'):
            importance = pd.DataFrame({
                'feature': feature_names,
                'coefficient': np.abs(self.model.coef_)
            }).sort_values('coefficient', ascending=False)
            
            print("\nüîç –¢–û–ü-10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ø–æ –º–æ–¥—É–ª—é –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞):")
            print(importance.head(10).to_string(index=False))
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        joblib.dump({
            'model': self.model,
            'scaler': self.scaler,
            'feature_names': feature_names
        }, self.model_path)
        print(f"  ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {self.model_path}")
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        y_pred_train = self.model.predict(X_scaled)
        train_mae = np.mean(np.abs(y_pred_train - y))
        train_acc = np.mean(np.abs(y_pred_train - y) <= 2) * 100
        
        metrics = {
            'cv_accuracy': np.mean(cv_scores),
            'train_accuracy': train_acc,
            'train_mae': train_mae,
            'n_features': len(feature_names),
            'n_samples': len(X)
        }
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        with open(self.artifacts_dir / "simple_metrics.json", 'w') as f:
            json.dump(metrics, f, indent=2)
        
        print("\n" + "="*70)
        print(f"‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
        print(f"  CV Accuracy ¬±2d: {metrics['cv_accuracy']:.1f}%")
        print(f"  Train Accuracy ¬±2d: {metrics['train_accuracy']:.1f}%")
        print(f"  Train MAE: {metrics['train_mae']:.2f} –¥–Ω–µ–π")
        print("="*70)
        
        return metrics
    
    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏."""
        if not self.model_path.exists():
            raise FileNotFoundError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞! –ó–∞–ø—É—Å—Ç–∏—Ç–µ train()")
        
        data = joblib.load(self.model_path)
        self.model = data['model']
        self.scaler = data['scaler']
        self.feature_names = data['feature_names']
    
    def predict(self, 
                storage_id: str,
                stack_id: str,
                max_temp: float,
                storage_days: int = 30,
                mass_tons: float = 5000,
                humidity: float = 50,
                air_temp: float = 15,
                wind_speed: float = 3,
                precipitation: float = 0) -> Dict[str, Any]:
        """
        –ü—Ä–æ—Å—Ç–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ.
        
        Args:
            storage_id: ID —Å–∫–ª–∞–¥–∞
            stack_id: ID —à—Ç–∞–±–µ–ª—è
            max_temp: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —à—Ç–∞–±–µ–ª—è (¬∞C)
            storage_days: –î–Ω–µ–π —Ö—Ä–∞–Ω–µ–Ω–∏—è
            mass_tons: –ú–∞—Å—Å–∞ (—Ç–æ–Ω–Ω)
            humidity: –í–ª–∞–∂–Ω–æ—Å—Ç—å (%)
            air_temp: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≤–æ–∑–¥—É—Ö–∞ (¬∞C)
            wind_speed: –°–∫–æ—Ä–æ—Å—Ç—å –≤–µ—Ç—Ä–∞ (–º/—Å)
            precipitation: –û—Å–∞–¥–∫–∏ (–º–º)
        """
        if self.model is None:
            self.load_model()
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        if max_temp < 0 or max_temp > 200:
            raise ValueError(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: {max_temp}¬∞C")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞
        df = pd.DataFrame([{
            'max_temp': max_temp,
            'storage_days': storage_days,
            'mass_tons': mass_tons,
            'humidity': humidity,
            'air_temp': air_temp,
            'wind_speed': wind_speed,
            'precipitation': precipitation
        }])
        
        # –ü—Ä–∏–∑–Ω–∞–∫–∏
        df = self.create_features(df)
        X = df[self.feature_names].fillna(0)
        X_scaled = self.scaler.transform(X)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        days_pred = self.model.predict(X_scaled)[0]
        days_pred = max(0, days_pred)  # –ù–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º
        
        # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (–Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã)
        if max_temp > 60:
            confidence = 0.9
        elif max_temp > 45:
            confidence = 0.75
        elif max_temp > 35:
            confidence = 0.6
        else:
            confidence = 0.4
        
        # –£—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞
        if days_pred < 3:
            risk = "–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π"
            risk_color = "red"
        elif days_pred < 7:
            risk = "–≤—ã—Å–æ–∫–∏–π"
            risk_color = "orange"
        elif days_pred < 14:
            risk = "—Å—Ä–µ–¥–Ω–∏–π"
            risk_color = "yellow"
        elif days_pred < 30:
            risk = "–Ω–∏–∑–∫–∏–π"
            risk_color = "green"
        else:
            risk = "–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π"
            risk_color = "gray"
        
        # –î–∞—Ç–∞ –≤–æ–∑–≥–æ—Ä–∞–Ω–∏—è
        fire_date = datetime.now() + timedelta(days=int(days_pred))
        
        return {
            'storage_id': storage_id,
            'stack_id': stack_id,
            'days_to_fire': round(days_pred, 1),
            'fire_date': fire_date.strftime('%Y-%m-%d'),
            'confidence': round(confidence, 2),
            'risk_level': risk,
            'risk_color': risk_color,
            'max_temp': max_temp,
            'storage_days': storage_days
        }

