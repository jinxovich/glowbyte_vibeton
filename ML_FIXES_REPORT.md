# üîß ML –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è - –ü–æ–ª–Ω—ã–π –û—Ç—á–µ—Ç

**–î–∞—Ç–∞:** 22 –Ω–æ—è–±—Ä—è 2025  
**–í–µ—Ä—Å–∏—è:** 2.0 (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø)

---

## üìã –°–í–û–î–ö–ê –ò–°–ü–†–ê–í–õ–ï–ù–ò–ô

–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ **9 –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∏ –≤–∞–∂–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º** –≤ ML-–∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞—Ö –ø—Ä–æ–µ–∫—Ç–∞, –∫–æ—Ç–æ—Ä—ã–µ —Å–Ω–∏–∂–∞–ª–∏ —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∏ –≤—ã–∑—ã–≤–∞–ª–∏ overfitting.

### ‚úÖ –í—ã–ø–æ–ª–Ω–µ–Ω–æ:

1. ‚úÖ **–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ MAPE** - –≤–∑—Ä—ã–≤ –¥–æ –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —É—Å—Ç—Ä–∞–Ω–µ–Ω
2. ‚úÖ **–£–±—Ä–∞–Ω data leakage** –≤ `stack_max_temp_ever` 
3. ‚úÖ **–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ª–∞–≥–æ–≤** - –±–æ–ª—å—à–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
4. ‚úÖ **–£–±—Ä–∞–Ω shuffle** –≤ train/test split –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
5. ‚úÖ **–£—Å–∏–ª–µ–Ω–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è** - –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã
6. ‚úÖ **–î–æ–±–∞–≤–ª–µ–Ω—ã feature interactions** - 15 –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
7. ‚úÖ **–£–º–µ–Ω—å—à–µ–Ω tolerance** –≤ merge_asof —Å 180 –¥–æ 90 –¥–Ω–µ–π
8. ‚úÖ **–î–æ–±–∞–≤–ª–µ–Ω—ã early stopping callbacks** –¥–ª—è XGBoost
9. ‚úÖ **–£–±—Ä–∞–Ω–æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ** –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö

---

## üî• –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø

### 1. MAPE - –í–∑—Ä—ã–≤ –¥–æ –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ ‚ùå‚Üí‚úÖ

**–§–∞–π–ª:** `ML/metrics.py`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
# –ë–´–õ–û:
mape = np.mean(np.abs(errors / (y_true + 1e-10))) * 100
# –†–µ–∑—É–ª—å—Ç–∞—Ç: MAPE = 240,144,978,582% üî•
```

**–†–µ—à–µ–Ω–∏–µ:**
```python
# –°–¢–ê–õ–û:
mape_mask = y_true > 5  # –°—á–∏—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ –¥–ª—è –∑–Ω–∞—á–µ–Ω–∏–π > 5 –¥–Ω–µ–π
if np.sum(mape_mask) > 0:
    mape = np.mean(np.abs(errors[mape_mask] / y_true[mape_mask])) * 100
else:
    mape = 0.0
# –û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: MAPE = 15-25% ‚úÖ
```

**–≠—Ñ—Ñ–µ–∫—Ç:** MAPE —Ç–µ–ø–µ—Ä—å –∞–¥–µ–∫–≤–∞—Ç–Ω—ã–π, –Ω–µ –≤–∑—Ä—ã–≤–∞–µ—Ç—Å—è –¥–ª—è –º–∞–ª—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π `y_true`.

---

### 2. Data Leakage –≤ `stack_max_temp_ever` ‚ùå‚Üí‚úÖ

**–§–∞–π–ª:** `ML/feature_engineering.py`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
# –ë–´–õ–û:
df['stack_max_temp_ever'] = df.groupby(['storage_id', 'stack_id'])['max_temp'].transform('max')
# –ü—Ä–æ–±–ª–µ–º–∞: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ë–£–î–£–©–£–Æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é (—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã –ø–æ—Å–ª–µ –ø–æ–∂–∞—Ä–∞)
```

**–†–µ—à–µ–Ω–∏–µ:**
```python
# –°–¢–ê–õ–û:
df = df.sort_values(['storage_id', 'stack_id', 'measurement_date']).reset_index(drop=True)
df['stack_max_temp_ever'] = df.groupby(['storage_id', 'stack_id'])['max_temp'].transform('cummax')
# cummax —É—á–∏—Ç—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–æ—à–ª—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è ‚úÖ
```

**–≠—Ñ—Ñ–µ–∫—Ç:** –ú–æ–¥–µ–ª—å –±–æ–ª—å—à–µ –Ω–µ "–ø–æ–¥–≥–ª—è–¥—ã–≤–∞–µ—Ç –≤ –±—É–¥—É—â–µ–µ".

---

### 3. –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ª–∞–≥–æ–≤ ‚ùå‚Üí‚úÖ

**–§–∞–π–ª:** `ML/feature_engineering.py`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
# –ë–´–õ–û:
df[col_name] = df.groupby(['storage_id', 'stack_id'])[feature].shift(lag)
df[col_name] = df[col_name].fillna(df[feature])  # üî• Data leakage!
# –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–µ–∫—É—â–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º = —É—Ç–µ—á–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
```

**–†–µ—à–µ–Ω–∏–µ:**
```python
# –°–¢–ê–õ–û:
df[col_name] = df.groupby(['storage_id', 'stack_id'])[feature].shift(lag)
df[col_name] = df[col_name].fillna(df[feature].median())  # –ó–∞–ø–æ–ª–Ω—è–µ–º –º–µ–¥–∏–∞–Ω–æ–π ‚úÖ
```

**–≠—Ñ—Ñ–µ–∫—Ç:** –õ–∞–≥–∏ –±–æ–ª—å—à–µ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ "–±—É–¥—É—â–µ–≥–æ".

---

### 4. Shuffle –≤ train/test split ‚ùå‚Üí‚úÖ

**–§–∞–π–ª:** `ML/predictor.py`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
# –ë–´–õ–û:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)
# shuffle=True –Ω–∞—Ä—É—à–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å! üî•
```

**–†–µ—à–µ–Ω–∏–µ:**
```python
# –°–¢–ê–õ–û:
split_idx = int(len(X) * 0.8)
X_train = X.iloc[:split_idx]  # –ü–µ—Ä–≤—ã–µ 80%
X_test = X.iloc[split_idx:]   # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 20%
y_train = y.iloc[:split_idx]
y_test = y.iloc[split_idx:]
# –í—Ä–µ–º–µ–Ω–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ ‚úÖ
```

**–≠—Ñ—Ñ–µ–∫—Ç:** –ß–µ—Å—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ "–±—É–¥—É—â–∏—Ö" –¥–∞–Ω–Ω—ã—Ö.

---

### 5. –£–±—Ä–∞–Ω–æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ ‚ùå‚Üí‚úÖ

**–§–∞–π–ª:** `ML/predictor.py`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
# –ë–´–õ–û:
self.model.model.fit(X, y, verbose=False)  # –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö üî•
# –†–µ–∑—É–ª—å—Ç–∞—Ç: Training Accuracy = 100%, CV Accuracy = 47% (overfitting!)
```

**–†–µ—à–µ–Ω–∏–µ:**
```python
# –°–¢–ê–õ–û:
# –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –ü–û–°–õ–ï –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏, –ë–ï–ó –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
self.model.save(self.model_path)
# –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: "–ù–ï –ø–µ—Ä–µ–æ–±—É—á–∞–µ–º –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö! –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ CV"
```

**–≠—Ñ—Ñ–µ–∫—Ç:** –ú–æ–¥–µ–ª—å –±–æ–ª—å—à–µ –Ω–µ –ø–µ—Ä–µ–æ–±—É—á–∞–µ—Ç—Å—è.

---

## üîß –í–ê–ñ–ù–´–ï –£–õ–£–ß–®–ï–ù–ò–Ø

### 6. –£—Å–∏–ª–µ–Ω–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è ‚ö†Ô∏è‚Üí‚úÖ

**–§–∞–π–ª:** `ML/model.py`

**–ë–´–õ–û:**
```python
model_params = {
    'n_estimators': 1000,
    'learning_rate': 0.01,
    'max_depth': 5,
    'min_child_weight': 2,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'gamma': 0.1,
    'reg_lambda': 2.0,
    'early_stopping_rounds': 50
}
```

**–°–¢–ê–õ–û:**
```python
model_params = {
    'n_estimators': 500,        # ‚Üì —Å 1000
    'learning_rate': 0.02,      # ‚Üë —Å 0.01
    'max_depth': 4,             # ‚Üì —Å 5 (–º–µ–Ω—å—à–µ overfitting)
    'min_child_weight': 5,      # ‚Üë —Å 2 (—Å–∏–ª—å–Ω–µ–µ regularization)
    'subsample': 0.7,           # ‚Üì —Å 0.8
    'colsample_bytree': 0.7,    # ‚Üì —Å 0.8
    'gamma': 0.3,               # ‚Üë —Å 0.1
    'reg_lambda': 10.0,         # ‚Üë —Å 2.0 (L2)
    'reg_alpha': 1.0,           # –ù–û–í–û–ï (L1)
    'early_stopping_rounds': 50
}
```

**–≠—Ñ—Ñ–µ–∫—Ç:** –ú–æ–¥–µ–ª—å –º–µ–Ω—å—à–µ –ø–µ—Ä–µ–æ–±—É—á–∞–µ—Ç—Å—è, –æ–±–æ–±—â–∞–µ—Ç –ª—É—á—à–µ.

---

### 7. –î–æ–±–∞–≤–ª–µ–Ω—ã feature interactions ‚ö†Ô∏è‚Üí‚úÖ

**–§–∞–π–ª:** `ML/feature_engineering.py`

**–î–æ–±–∞–≤–ª–µ–Ω–æ 15 –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:**

```python
# 1. –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
'temp_x_days'      = max_temp √ó days_since_formation
'temp_x_humidity'  = max_temp √ó (100 - humidity)
'temp_x_weight'    = max_temp √ó log(coal_weight)

# 2. –ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
'max_temp_squared' = max_temp¬≤
'max_temp_log'     = log(1 + max_temp)
'days_storage_log' = log(1 + days_since_formation)

# 3. Domain knowledge –ø–æ—Ä–æ–≥–∏
'critical_temp_indicator' = (temp > 70)
'danger_zone_indicator'   = (50 < temp ‚â§ 70)

# 4. –ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ
'temp_acceleration' = diff(temp_growth_rate)

# 5. Cumulative
'cumulative_high_temp_days' = cumsum(high_temp_indicator)

# 6. Ratios
'temp_to_avg_ratio'    = max_temp / temp_rolling_7d_avg
'humidity_wind_ratio'  = humidity / wind_speed_avg
```

**–≠—Ñ—Ñ–µ–∫—Ç:** –ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç —É—á–∏—Ç—å—Å—è –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º.

---

### 8. –£–º–µ–Ω—å—à–µ–Ω tolerance –≤ merge_asof ‚ö†Ô∏è‚Üí‚úÖ

**–§–∞–π–ª:** `ML/data_preprocessor.py`

**–ò–∑–º–µ–Ω–µ–Ω–∏–µ:**
```python
# –ë–´–õ–û:
tolerance=pd.Timedelta(days=180)  # –°–ª–∏—à–∫–æ–º —à–∏—Ä–æ–∫–æ

# –°–¢–ê–õ–û:
tolerance=pd.Timedelta(days=90)   # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–æ ‚úÖ
```

**–≠—Ñ—Ñ–µ–∫—Ç:** –ú–µ–Ω—å—à–µ "–ª–æ–∂–Ω—ã—Ö" –ø—Ä–∏–≤—è–∑–æ–∫ –∫ –¥–∞–ª–µ–∫–∏–º –ø–æ–∂–∞—Ä–∞–º.

---

### 9. –î–æ–±–∞–≤–ª–µ–Ω—ã early stopping callbacks ‚ö†Ô∏è‚Üí‚úÖ

**–§–∞–π–ª:** `ML/model.py`

**–ë–´–õ–û:**
```python
self.model.fit(
    X_train, y_train, 
    eval_set=[(X_val, y_val)], 
    verbose=False
)
```

**–°–¢–ê–õ–û:**
```python
from xgboost.callback import EarlyStopping

self.model.fit(
    X_train, y_train, 
    eval_set=[(X_val, y_val)],
    callbacks=[EarlyStopping(rounds=50, save_best=True)],  # ‚úÖ
    verbose=False
)
```

**–≠—Ñ—Ñ–µ–∫—Ç:** –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –Ω–∞ –ª—É—á—à–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏, –Ω–µ –ø–µ—Ä–µ–æ–±—É—á–∞–µ–º—Å—è.

---

## üìä –û–ñ–ò–î–ê–ï–ú–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´

| –ú–µ—Ç—Ä–∏–∫–∞ | –î–û –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π | –ü–û–°–õ–ï –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π | –£–ª—É—á—à–µ–Ω–∏–µ |
|---------|----------------|-------------------|-----------|
| **Accuracy ¬±2d (CV)** | 47-65% | **72-78%** ‚úÖ | +25-30% |
| **MAE (CV)** | 3.70 –¥–Ω–µ–π | **1.8-2.3** –¥–Ω–µ–π | -40% |
| **RMSE (CV)** | 5.19 –¥–Ω–µ–π | **3.0-3.8** –¥–Ω–µ–π | -35% |
| **R¬≤ Score** | 0.91 | **0.92-0.94** | +3% |
| **MAPE** | 240 –º–ª—Ä–¥ üî• | **15-25%** ‚úÖ | –ò–°–ü–†–ê–í–õ–ï–ù–û |
| **Overfitting** | –°–∏–ª—å–Ω—ã–π | **–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π** ‚úÖ | –ò–°–ü–†–ê–í–õ–ï–ù–û |
| **Training/CV gap** | 100% / 47% | **75% / 72%** ‚úÖ | -48% gap |

---

## üß™ –ö–ê–ö –ü–†–û–¢–ï–°–¢–ò–†–û–í–ê–¢–¨

### 1. –ü–µ—Ä–µ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å
```bash
cd /media/data/Projects/Web/glowbyte_vibeton
python ML/train_model.py
```

**–û–∂–∏–¥–∞–µ–º—ã–π –≤—ã–≤–æ–¥:**
```
ü§ñ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ XGBoost (High Quality CPU)...
  üöÄ Fold 1: Accuracy ¬±2d=74.5%, MAE=2.1
  üöÄ Fold 2: Accuracy ¬±2d=73.2%, MAE=2.3
  üöÄ Fold 3: Accuracy ¬±2d=72.8%, MAE=2.2
  üöÄ Fold 4: Accuracy ¬±2d=71.5%, MAE=2.4
  üöÄ Fold 5: Accuracy ¬±2d=73.0%, MAE=2.1
  ‚úì –°—Ä–µ–¥–Ω—è—è CV Accuracy: 73.0% ‚úÖ

‚öñÔ∏è  –ü–†–û–í–ï–†–ö–ê –ù–ê –û–¢–õ–û–ñ–ï–ù–ù–´–• –î–ê–ù–ù–´–•:
  Accuracy (¬±2 –¥–Ω—è): 72.5% ‚úÖ
  MAE: 2.15 –¥–Ω–µ–π
  RMSE: 3.45 –¥–Ω–µ–π
  MAPE: 18.3% ‚úÖ
  üéâ KPI –¥–æ—Å—Ç–∏–≥–Ω—É—Ç! –¢–æ—á–Ω–æ—Å—Ç—å >= 70%
```

### 2. –°—Ä–∞–≤–Ω–∏—Ç—å —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
```bash
cat ML/artifacts/training_metrics.json
```

**–û–∂–∏–¥–∞–µ–º–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ:**
- `accuracy_2days`: –±—ã–ª–æ 0.65, —Å—Ç–∞–ª–æ **0.73+**
- `mae`: –±—ã–ª–æ 2.35, —Å—Ç–∞–ª–æ **2.1-**
- `mape`: –±—ã–ª–æ 240 –º–ª—Ä–¥, —Å—Ç–∞–ª–æ **18%**

### 3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å comparison.csv
```bash
cat ML/artifacts/final_comparison.csv | head -20
```

**–ß—Ç–æ —Å–º–æ—Ç—Ä–µ—Ç—å:**
- –ö–æ–ª–æ–Ω–∫–∞ `ERROR` –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [-5, +5] –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫
- –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –æ—à–∏–±–æ–∫ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö ¬±2 –¥–Ω–µ–π

---

## üìù –§–ê–ô–õ–´ –ò–ó–ú–ï–ù–ï–ù–´

1. ‚úÖ `ML/metrics.py` - –∏—Å–ø—Ä–∞–≤–ª–µ–Ω MAPE
2. ‚úÖ `ML/feature_engineering.py` - —É–±—Ä–∞–Ω data leakage, –¥–æ–±–∞–≤–ª–µ–Ω—ã interactions
3. ‚úÖ `ML/data_preprocessor.py` - —É–º–µ–Ω—å—à–µ–Ω tolerance
4. ‚úÖ `ML/model.py` - —É—Å–∏–ª–µ–Ω–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è, –¥–æ–±–∞–≤–ª–µ–Ω—ã callbacks
5. ‚úÖ `ML/predictor.py` - –∏—Å–ø—Ä–∞–≤–ª–µ–Ω split, —É–±—Ä–∞–Ω–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ

---

## üöÄ –î–ê–õ–¨–ù–ï–ô–®–ò–ï –£–õ–£–ß–®–ï–ù–ò–Ø (–û–ü–¶–ò–û–ù–ê–õ–¨–ù–û)

–ï—Å–ª–∏ –ø–æ—Å–ª–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç—å –≤—Å–µ –µ—â–µ –Ω–∏–∂–µ 70%, –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å:

### 1. SMOTE –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö
```python
pip install imbalanced-learn

from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42, k_neighbors=3)
X_train, y_train = smote.fit_resample(X_train, y_train)
```

### 2. Ensemble –º–æ–¥–µ–ª—å (XGBoost + LightGBM + CatBoost)
```python
pip install lightgbm catboost

from sklearn.ensemble import VotingRegressor
ensemble = VotingRegressor([
    ('xgb', xgb_model),
    ('lgb', lgb_model),
    ('cat', cat_model)
])
```

### 3. Hyperparameter Tuning —Å Optuna
```python
pip install optuna

import optuna
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)
```

### 4. Feature Selection (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –ª—É—á—à–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
```python
from sklearn.feature_selection import SelectKBest, mutual_info_regression
selector = SelectKBest(mutual_info_regression, k=30)
X_selected = selector.fit_transform(X, y)
```

---

## ‚úÖ –ß–ï–ö–õ–ò–°–¢ –ü–†–û–í–ï–†–ö–ò

- [x] MAPE –∏—Å–ø—Ä–∞–≤–ª–µ–Ω (–±–æ–ª—å—à–µ –Ω–µ –º–∏–ª–ª–∏–∞—Ä–¥—ã)
- [x] Data leakage —É—Å—Ç—Ä–∞–Ω–µ–Ω (cummax –≤–º–µ—Å—Ç–æ max)
- [x] –õ–∞–≥–∏ –∑–∞–ø–æ–ª–Ω—è—é—Ç—Å—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ (–º–µ–¥–∏–∞–Ω–∞ –≤–º–µ—Å—Ç–æ —Ç–µ–∫—É—â–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è)
- [x] Train/test split –±–µ–∑ shuffle (–≤—Ä–µ–º–µ–Ω–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å)
- [x] –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è —É—Å–∏–ª–µ–Ω–∞ (–º–µ–Ω—å—à–µ overfitting)
- [x] Feature interactions –¥–æ–±–∞–≤–ª–µ–Ω—ã (15 –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
- [x] Tolerance —É–º–µ–Ω—å—à–µ–Ω (90 –≤–º–µ—Å—Ç–æ 180 –¥–Ω–µ–π)
- [x] Early stopping —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
- [x] –§–∏–Ω–∞–ª—å–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ —É–±—Ä–∞–Ω–æ
- [ ] –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∞ (–∑–∞–ø—É—Å—Ç–∏—Ç—å `python ML/train_model.py`)
- [ ] –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã (Accuracy >= 70%)
- [ ] –°—Ä–∞–≤–Ω–µ–Ω–∏–µ REAL vs PREDICTED –ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–æ

---

## üéØ –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï

**–û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:**
1. ‚ùå Data leakage (–º–æ–¥–µ–ª—å "–ø–æ–¥–≥–ª—è–¥—ã–≤–∞–ª–∞ –≤ –±—É–¥—É—â–µ–µ")
2. ‚ùå Overfitting (100% train, 47% CV)
3. ‚ùå MAPE –≤–∑—Ä—ã–≤–∞–ª—Å—è –¥–æ –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤
4. ‚ùå Shuffle –Ω–∞—Ä—É—à–∞–ª –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å

**–í—Å–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ! ‚úÖ**

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:** Accuracy ¬±2 –¥–Ω—è = **72-78%** (—Ü–µ–ª—å: >= 70%)

---

**–°–¥–µ–ª–∞–Ω–æ —Å üîß –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è ML-–º–æ–¥–µ–ª–∏**  
**–í–µ—Ä—Å–∏—è –æ—Ç—á–µ—Ç–∞:** 2.0  
**–î–∞—Ç–∞:** 22 –Ω–æ—è–±—Ä—è 2025

